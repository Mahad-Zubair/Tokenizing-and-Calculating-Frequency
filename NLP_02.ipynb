{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZBSdu6IMPxpD",
        "outputId": "058ecf72-4f29-4bf2-bdb0-1de425acf51d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter 'W' to Tokenize Words or Enter 'C' to Tokenize Characters: c\n",
            "['T', 'h', 'i', 's', ' ', 't', 'e', 'x', 't', ' ', 'f', 'i', 'l', 'e', ' ', 'i', 's', ' ', 'm', 'a', 'd', 'e', ' ', 't', 'o', ' ', 'a', 'c', 'h', 'i', 'e', 'v', 'e', ' ', 't', 'h', 'e', ' ', 't', 'a', 's', 'k', ' ', 'a', 's', 's', 'i', 'g', 'n', 'e', 'd', ' ', 'b', 'y', ' ', 'o', 'u', 'r', ' ', 'i', 'n', 's', 't', 'r', 'u', 'c', 't', 'o', 'r', ' ', ' ', 'I', 't', ' ', 's', 'h', 'o', 'u', 'l', 'd', ' ', 'c', 'o', 'n', 't', 'a', 'i', 'n', ' ', 'r', 'a', 'n', 'd', 'o', 'm', ' ', 'w', 'o', 'r', 'd', 's', ' ', 'o', 'r', ' ', 's', 'e', 'n', 't', 'e', 'n', 'c', 'e', 's', ' ', ' ', 'i', 't', ' ', 'c', 'a', 'n', ' ', 'b', 'e', ' ', 's', 'e', 'n', 't', 'e', 'n', 'c', 'e', 's', ' ', 'o', 'r', ' ', 'w', 'o', 'r', 'd', 's', ' ', 'N', 'L', 'P', ' ', 'i', 's', ' ', 't', 'h', 'e', ' ', 's', 'u', 'b', 'j', 'e', 'c', 't', ' ', ' ', 't', 'h', 'i', 's', ' ', 'a', 's', 's', 'i', 'g', 'n', 'm', 'e', 'n', 't', ' ', 'i', 's', ' ', 'f', 'o', 'r', ' ', 'N', 'L', 'P', ' ', ' ', 'M', 'y', ' ', 'n', 'a', 'm', 'e', ' ', 'i', 's', ' ', 'M', 'a', 'h', 'a', 'd', ' ', 'Z', 'u', 'b', 'a', 'i', 'r', ' ', ' ', 'o', 'f', ' ', 's', 'e', 'c', 't', 'i', 'o', 'n', ' ', 'B', ' ', 'a', 'n', 'd', ' ', 's', 'e', 'v', 'e', 'n', 't', 'h', ' ', 's', 'e', 'm', 'e', 's', 't', 'e', 'r', ' '] \n",
            "\n",
            "  occurs: 51\n",
            "s occurs: 24\n",
            "e occurs: 24\n",
            "t occurs: 20\n",
            "n occurs: 18\n",
            "i occurs: 16\n",
            "a occurs: 13\n",
            "o occurs: 13\n",
            "r occurs: 11\n",
            "h occurs: 8\n",
            "d occurs: 8\n",
            "c occurs: 8\n",
            "m occurs: 7\n",
            "b occurs: 5\n",
            "u occurs: 5\n",
            "l occurs: 4\n",
            "f occurs: 3\n",
            "v occurs: 2\n",
            "g occurs: 2\n",
            "y occurs: 2\n",
            "w occurs: 2\n",
            "p occurs: 2\n",
            "x occurs: 1\n",
            "k occurs: 1\n",
            "j occurs: 1\n",
            "z occurs: 1\n"
          ]
        }
      ],
      "source": [
        "file = open(\"/content/drive/MyDrive/NLP-TEXT.txt\")\n",
        "data = file.read()\n",
        "\n",
        "##--------------Dealing with Special Characters---------------------##\n",
        "special_chars = \",.?!(){}[]\"\n",
        "for char in special_chars:\n",
        "    if char in data:\n",
        "        data = data.replace(char, \" \")\n",
        "\n",
        "##--------------Tokenization---------------------##\n",
        "def tokenize_words(data):\n",
        "    return data.split()\n",
        "\n",
        "def tokenize_chars(data):\n",
        "    return list(data)\n",
        "\n",
        "##-----------------Frequency Calculator------------------##\n",
        "def word_freq(data):\n",
        "    words = data.lower().split()\n",
        "    freq = {}\n",
        "    for word in words:\n",
        "        freq[word] = freq.get(word, 0) + 1\n",
        "    sorted_words= sorted(freq.items(), key=lambda item: item[1], reverse=True)\n",
        "    for item in sorted_words:\n",
        "        print(f\"{item[0]} occurs: {item[1]} times\")\n",
        "    return freq\n",
        "\n",
        "def char_freq(data):\n",
        "    freq = {}\n",
        "    for char in data:\n",
        "        if char.isalpha():\n",
        "            freq[char.lower()] = freq.get(char.lower(), 0) + 1\n",
        "        else:\n",
        "             freq[char] = freq.get(char, 0) + 1\n",
        "    sorted_char = sorted(freq.items(), key=lambda item: item[1], reverse=True)\n",
        "    for key, value in sorted_char:\n",
        "        print(f\"{key} occurs: {value}\")\n",
        "    return freq\n",
        "#------------------------------------------------------------##\n",
        "option = input(\"Enter 'W' to Tokenize Words or Enter 'C' to Tokenize Characters: \")\n",
        "\n",
        "if option.lower() == \"w\":\n",
        "    tokens = tokenize_words(data)\n",
        "    print(tokens,\"\\n\")\n",
        "    word_freq(data)\n",
        "\n",
        "\n",
        "elif option.lower() == \"c\":\n",
        "    tokens = tokenize_chars(data)\n",
        "    print(tokens,\"\\n\")\n",
        "    char_freq(data)\n",
        "else:\n",
        "    print(\"INVALID OPTION SELECTED!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4NdlEFzuwTVF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}